# Minesweeper AI Training Project

Deep reinforcement learning to train AI to play Minesweeper.

## Project Structure

```
mineclear/
├── environment/      # Minesweeper game environment simulator
│   ├── minesweeper.py         # Gymnasium-based Minesweeper environment
│   └── minesweeper_solver.py  # Professional Minesweeper solver for data generation
├── agent/            # Reinforcement learning agents
│   ├── dqn_agent.py  # DQN agent implementation
│   ├── ppo_agent.py  # PPO agent implementation
│   └── human_reasoning.py  # Human reasoning module
├── training/         # Training logic and workflow
│   ├── dqn_trainer.py # DQN trainer
│   └── ppo_trainer.py # PPO trainer
├── visualization/    # Visualization of AI decision process
├── real_game/        # Real game interface
│   └── minesweeper_game.py # Playable Minesweeper game implementation
├── utils/            # Helper functions
├── main.py           # Main program
├── test_project.py   # Test script
├── evaluate_models.py # Model evaluation script
└── requirements.txt  # Project dependencies
```

## Features

1. **Enhanced Minesweeper Environment**
   - Environment design based on OpenAI Gymnasium
   - Rich reward system, including strategy rewards and information gain rewards
   - Automatic prevention of first click on mine

2. **Advanced Reinforcement Learning Algorithms**
   - **Dueling DQN**: Separates state value function and advantage function
   - **Prioritized Experience Replay (PER)**: Assigns priority to experiences based on TD error
   - **Double DQN**: Reduces Q-value overestimation problems
   - **Deep Convolutional Network**: Deep convolutional network with attention mechanism
   - **Four-channel State Representation**: Enhanced pattern recognition capability
   - **Advanced Reward Shaping**: Customized reward signals based on game state

3. **Human Reasoning Capabilities**
   - Simulates human thought process in playing Minesweeper
   - Constraint Satisfaction Problem (CSP) based reasoning system
   - Three-layer reasoning strategy: single cell reasoning, overlapping constraint reasoning, and probability analysis
   - Seamless integration with neural networks, intelligently selecting decision methods

4. **Professional Minesweeper Solver**
   - Provides high-quality pre-training data
   - Implements various Minesweeper solving strategies
   - Standard solution for evaluation benchmarking

5. **Advanced Training System**
   - Curriculum learning: training flow from simple to complex
   - Self-supervised pre-training: using samples generated by professional solver
   - Memory mechanism: tracking cells determined to be safe
   - Automatic difficulty adjustment: increasing training difficulty based on win rate

6. **Method Comparison and Evaluation**
   - Comparison of pure human logic, pure DQN, and their combination
   - Detailed performance metrics: average reward, win rate, and reasoning usage rate
   - Automatic generation of comparison reports

7. **Visualization and Game Implementation**
   - Interactive Minesweeper game implementation, supporting human and AI players
   - Real-time AI assistance
   - Detailed game statistics and result analysis

8. **Complete Test Suite**
   - Automated testing of environment, agent, and training modules
   - Performance benchmarking
   - Automatic generation of test reports

## Installation

```bash
pip install -r requirements.txt
```

## Usage

### Training Mode

```bash
python main.py --mode train --num_episodes 10000 --initial_difficulty 3 --render --verbose
```

Advanced training parameters:
```bash
python main.py --mode train --num_episodes 10000 --batch_size 128 --learning_rate 0.0001 --use_human_reasoning True --disable_reasoning False
```

Parameter description:
- `--algorithm`: Choose training algorithm, supports 'dqn' or 'ppo'
- `--num_episodes`: Number of training episodes
- `--initial_difficulty`: Initial difficulty (number of mines)
- `--disable_reasoning`: Disable human reasoning capability
- `--difficulty_threshold`: Win rate threshold for increasing difficulty (default 0.7)

### Evaluation Mode

```bash
python main.py --mode eval --model_path models/dqn_model_ep8100_diff4.pth --eval_episodes 50
```

Parameter description:
- `--model_path`: Path to load the pre-trained model
- `--eval_episodes`: Number of evaluation episodes
- `--eval_difficulty`: Evaluation difficulty level
- `--disable_reasoning`: Disable human reasoning capability
- `--stochastic`: Use stochastic policy instead of deterministic policy

### Method Comparison

```bash
python main.py --mode compare --model_path models/dqn_model_ep8100_diff4.pth --compare_episodes 50 --eval_difficulty 5
```

Batch model evaluation:
```bash
python evaluate_models.py
```

Parameter description:
- `--model_path`: Path to load the pre-trained model
- `--compare_episodes`: Number of evaluation episodes per method
- `--eval_difficulty`: Evaluation difficulty level

### Game Mode

```bash
python main.py --mode play --width 9 --height 9 --num_mines 10 --model_path models/dqn_model_ep8100_diff4.pth
```

Custom control keys:
```bash
python main.py --mode play --width 9 --height 9 --num_mines 10 --ai-key=s --reset-key=n --quit-key=e
```

Parameter description:
- `--width`: Game board width
- `--height`: Game board height
- `--num_mines`: Number of mines
- `--model_path`: Path to load the AI model
- `--ai-key`: AI assistance key (default 'a')
- `--reset-key`: Reset game key (default 'r')
- `--quit-key`: Quit game key (default 'q')

### Test Mode

```bash
python test_project.py [--all] [--dependencies] [--environment] [--agent] [--training] [--cli] [--benchmark]
```

Parameter description:
- `--all`: Run all tests (default)
- `--dependencies`: Test project dependencies only
- `--environment`: Test environment module only
- `--agent`: Test agent module only
- `--training`: Test training module only
- `--cli`: Test command line interface only
- `--benchmark`: Run performance benchmark only
- `--seed`: Set random seed (default 42)

## Recent Improvements

### 1. Advanced Reinforcement Learning Algorithm Implementation

- **Dueling DQN Architecture**: Decomposes Q-value into state value function V(s) and advantage function A(s,a), more efficiently learning state values
- **Prioritized Experience Replay (PER)**: Assigns higher sampling probability to important experiences based on TD error, improving learning efficiency
- **Advanced Reward Shaping Strategies**:
  - Chain reaction reward: Extra reward based on the number of cells opened in one operation
  - Edge exploration reward: Encourages exploration behavior at the edge of known regions
  - Logical reasoning reward: Extra reward for decisions that conform to logical reasoning
  - Progress-based failure penalty: Adjusts the severity of failure penalties based on game progress

### 2. Improved Human Reasoning Module

- **CSP Solver Optimization**: More efficient constraint satisfaction problem solver
- **Probability Model Improvement**: More accurate mine probability calculation method
- **Reasoning-Network Collaboration Mechanism**: Intelligently switches between deterministic reasoning and probabilistic decision-making
- **Edge Cell Recognition**: Identifies and prioritizes exploration of information-rich edge cells

### 3. Enhanced Game Experience

- **Interactive Game Interface**: Complete Minesweeper game implemented with pygame
- **AI Assistance**: One-key access to AI's recommended next move
- **Custom Control Keys**: Customize game control keys via command line parameters
- **In-game Status Display**: Real-time display of mine count and game status

### 4. Training and Evaluation Improvements

- **Automatic Difficulty Adjustment**: Dynamically adjusts training difficulty based on win rate
- **Batch Model Evaluation**: Use `evaluate_models.py` script to evaluate multiple models in batch
- **Detailed Statistics of Different Decision Strategies**: Records and analyzes the effects of reasoning vs. neural network decisions
- **Training Process Visualization**: Displays real-time training progress and performance metrics





## Contributing

Contributions are welcome, including code contributions, issue reporting, or improvement suggestions!

## License

This project is open-sourced under the MIT License. 